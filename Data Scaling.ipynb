{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6861d6fc",
   "metadata": {},
   "source": [
    "I’ve been reading information about scaling inputs before we create a model (This is something that I do directly in some of the regularization functions with them that allow me this with a parameter), then, here I sent what I concluded:\n",
    "•\tIn standard linear regression is not necessary scale the input data.\n",
    "•\tIn regularization, clustering and PCA it’s important scaling the data before fitting.\n",
    "•\tWe must scale the data after we have done training-test split, to avoid information leak of the training in the test (often leading to overly optimistic results during evaluation).\n",
    "•\tWe scale the test data with the information of the training data .\n",
    "•\tWe can scale the input and output variables, but normally, we scale only input variables.\n",
    "•\tThere are many methods to scale the data, but the most known ones are normalization and standardization, but the last one requires a normal distribution of every variable, we can try both and evaluate results.\n",
    "•\tIf we expect some outliers, RobustScaler works better.\n",
    "•\tUse pipelines option of sklearn when we have to do more than one step in preprocessing the data before fitting.\n",
    "This is the source of the online information.\n",
    "•\tWhen and Why to Standardize Your Data | Built In (https://builtin.com/data-science/when-and-why-standardize-your-data)\n",
    "•\tHow to use Data Scaling Improve Deep Learning Model Stability and Performance - MachineLearningMastery.com (https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)\n",
    "•\tWhen, Why, And How You Should Standardize Your Data (machinelearningcompass.com) (https://machinelearningcompass.com/dataset_optimization/standardization/)\n",
    "•\tStandardization in case of real-time predictions | element61 (https://www.element61.be/en/resource/standardization-case-real-time-predictions)\n",
    "•\tpredictive models - Standardization and prediction on new data - Cross Validated (stackexchange.com) (https://stats.stackexchange.com/questions/214331/standardization-and-prediction-on-new-data)\n",
    "•\tHow and why to Standardize your data: A python tutorial (https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a0d4698",
   "metadata": {},
   "source": [
    "# Data scaling\n",
    "In this notebook we will explore how to scale the data, something mandatory when we need to apply some machine learging methods like PCA, clustering, K-nearest neighboors, Support vector machine, etc...\n",
    "Differences in the scales across input variables may increase the difficulty of the problem being modeled. An example of this is that large input values (e.g. a spread of hundreds or thousands of units) can result in a model that learns large weight values. A model with large weight values is often unstable, meaning that it may suffer from poor performance during learning and sensitivity to input values resulting in higher generalization error. (https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)\n",
    " \n",
    "I will use scaling methods of Scikit-Learn and i will test the answer with manual procedure.\n",
    "Note: Most of the methods receive List of lists (Being each list one row) and Data frame, applying scaling through columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "548b69d3",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "\n",
    "Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1.\n",
    "\n",
    "Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data.\n",
    "\n",
    "A value is normalized as follows:\n",
    "$$\n",
    "x_{scaled} = (x - x_{min})/(x_{max} - x_{min})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02b8cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0e+02 1.0e-03]\n",
      " [8.0e+00 5.0e-02]\n",
      " [5.0e+01 5.0e-03]\n",
      " [8.8e+01 7.0e-02]\n",
      " [4.0e+00 1.0e-01]]\n",
      "[[1.         0.        ]\n",
      " [0.04166667 0.49494949]\n",
      " [0.47916667 0.04040404]\n",
      " [0.875      0.6969697 ]\n",
      " [0.         1.        ]]\n",
      "Minimos [4.e+00 1.e-03]\n",
      "Máximos [100.    0.1]\n"
     ]
    }
   ],
   "source": [
    "# example of a normalization\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# define data\n",
    "data = asarray([[100, 0.001],\n",
    " [8, 0.05],\n",
    " [50, 0.005],\n",
    " [88, 0.07],\n",
    " [4, 0.1]])\n",
    "print(data)\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)\n",
    "print('Minimos',scaler.data_min_)\n",
    "print('Máximos',scaler.data_max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5352a681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       a      b\n",
      "0  100.0  0.001\n",
      "1    8.0  0.050\n",
      "2   50.0  0.005\n",
      "3   88.0  0.070\n",
      "4    4.0  0.100\n",
      "[[1.         0.        ]\n",
      " [0.04166667 0.49494949]\n",
      " [0.47916667 0.04040404]\n",
      " [0.875      0.6969697 ]\n",
      " [0.         1.        ]]\n",
      "          a         b\n",
      "0  1.000000  0.000000\n",
      "1  0.041667  0.494949\n",
      "2  0.479167  0.040404\n",
      "3  0.875000  0.696970\n",
      "4  0.000000  1.000000\n",
      "Minimos [4.e+00 1.e-03]\n",
      "Máximos [100.    0.1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DataTable2 = pd.DataFrame(data,columns=['a','b'])\n",
    "scaled2 = scaler.fit_transform(DataTable2)\n",
    "scaledtable = pd.DataFrame(scaled2,columns=['a','b'])\n",
    "print(DataTable2)\n",
    "print(scaled2)\n",
    "print(scaledtable)\n",
    "print('Minimos',scaler.data_min_)\n",
    "print('Máximos',scaler.data_max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809dcbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "for i in DataTable2.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb279e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -1.        ]\n",
      " [-0.91666667 -0.01010101]\n",
      " [-0.04166667 -0.91919192]\n",
      " [ 0.75        0.39393939]\n",
      " [-1.          1.        ]]\n",
      "[[1.0e+02 1.0e-03]\n",
      " [8.0e+00 5.0e-02]\n",
      " [5.0e+01 5.0e-03]\n",
      " [8.8e+01 7.0e-02]\n",
      " [4.0e+00 1.0e-01]]\n"
     ]
    }
   ],
   "source": [
    "# create scaler with diferent minimum and maximum\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(data)\n",
    "# apply transform\n",
    "normalized = scaler.transform(data)\n",
    "print(normalized)\n",
    "# inverse transform\n",
    "inverse = scaler.inverse_transform(normalized)\n",
    "print(inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f6a410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.26398112 -1.16389967]\n",
      " [-1.06174414  0.12639634]\n",
      " [ 0.         -1.05856939]\n",
      " [ 0.96062565  0.65304778]\n",
      " [-1.16286263  1.44302493]]\n",
      "Media [5.00e+01 4.52e-02]\n",
      "Varianza [1.56480e+03 1.44216e-03]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data)\n",
    "standardized = scaler.transform(data)\n",
    "print(standardized)\n",
    "print('Media',scaler.mean_)\n",
    "print('Varianza',scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c5e6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1\n",
      "0  100.0  0.001\n",
      "1    8.0  0.050\n",
      "2   50.0  0.005\n",
      "3   88.0  0.070\n",
      "4    4.0  0.100\n",
      "          0         1\n",
      "0  1.263981 -1.163900\n",
      "1 -1.061744  0.126396\n",
      "2  0.000000 -1.058569\n",
      "3  0.960626  0.653048\n",
      "4 -1.162863  1.443025\n",
      "Media 0    50.0000\n",
      "1     0.0452\n",
      "dtype: float64\n",
      "Varianza 0    1564.800000\n",
      "1       0.001442\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\H279031\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3438: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "data_mean = np.mean(df)\n",
    "data_std = np.std(df)\n",
    "df_standardized = (df-data_mean)/data_std\n",
    "print(df_standardized)\n",
    "print('Media',data_mean)\n",
    "print('Varianza',data_std**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96f2245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.625      -0.75384615]\n",
      " [-0.525       0.        ]\n",
      " [ 0.         -0.69230769]\n",
      " [ 0.475       0.30769231]\n",
      " [-0.575       0.76923077]]\n",
      "escale [8.0e+01 6.5e-02]\n",
      "Mediana [50.    0.05]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(data)\n",
    "robust_scaled = scaler.transform(data)\n",
    "print(robust_scaled)\n",
    "print('escale',scaler.scale_)\n",
    "print('Mediana',scaler.center_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23859260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    80.000\n",
      "1     0.065\n",
      "dtype: float64\n",
      "       0         1\n",
      "0  0.625 -0.753846\n",
      "1 -0.525  0.000000\n",
      "2  0.000 -0.692308\n",
      "3  0.475  0.307692\n",
      "4 -0.575  0.769231\n"
     ]
    }
   ],
   "source": [
    "cuartiles = df.quantile([.25, .50, .75]).values\n",
    "cuartiles = pd.DataFrame(cuartiles)\n",
    "Mediana = cuartiles.iloc[1]\n",
    "Q1 = cuartiles.iloc[0]\n",
    "Q3 = cuartiles.iloc[2]\n",
    "RIC = Q3-Q1\n",
    "print(RIC)\n",
    "robust_scaled = (df-Mediana)/RIC\n",
    "print(robust_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
